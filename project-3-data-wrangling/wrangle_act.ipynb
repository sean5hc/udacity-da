{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tweepy\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's load the files into dataframes for use in the following assessment and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the image predictions file if we don't already have it...\n",
    "my_file = Path(\"/home/workspace/image-predictions_2.tsv\")\n",
    "if my_file.is_file():\n",
    "    # request the image predictions file from provided url\n",
    "    ip_r = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')\n",
    "    # write it to file\n",
    "    with open('/home/workspace/image-predictions.tsv', 'wb') as f:\n",
    "        f.write(ip_r.content)\n",
    "    \n",
    "# load the image predictions file into data\n",
    "image_predictions = pd.read_csv(\"/home/workspace/image-predictions.tsv\", delimiter = \"\\t\")\n",
    "\n",
    "# load the twitter archive into data\n",
    "twitter_archive = pd.read_csv(\"/home/workspace/twitter-archive-enhanced.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The code below was used to gather tweet json from the twitter API. It is commented out to prevent it from running again accidentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set credentials to the twitter api\n",
    "consumer_key = '7NlGqlD9Xn4BOveghhn5W4MED'\n",
    "consumer_secret = 'o8Yz1J1kKPouFWEf5bYp93omy1ULP27ha8MNzpPReC5TytJRv2'\n",
    "access_token = '917796802363084801-J0uEjWUuz1vlVbi7b2veIAeC34m6Zvn'\n",
    "access_secret = 'dXznm8gkBzrgngDK08qQsQuG4ayDm3VsiXZmA0fhZz3oe'\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# container for the json received from api\n",
    "tweet_json_list = []\n",
    "\n",
    "def fetch_tweets(status_id_list):\n",
    "    try:\n",
    "        tweet_list = api.statuses_lookup(status_id_list)\n",
    "    except tweepy.TweepError as e:\n",
    "        return 0\n",
    "    pass\n",
    "    for i in range(len(tweet_list)):\n",
    "        tweet_json_list.append(json.dumps(tweet_list[i]._json))\n",
    "\n",
    "# fetch the tweet data from Twitter API if we don't already have it...\n",
    "my_file = Path(\"/home/workspace/tweet_json.txt\")\n",
    "if my_file.is_file() == False:\n",
    "    # create a list of groups of tweet ids 100 ids in length each\n",
    "    counter = 0\n",
    "    tweet_id_groups = []\n",
    "    tweet_id_group = []\n",
    "    for index, row in twitter_archive.iterrows():\n",
    "        if counter < 99:\n",
    "            tweet_id_group.append(row['tweet_id'])\n",
    "            counter+=1\n",
    "        else:\n",
    "            g = list(tweet_id_group)\n",
    "            tweet_id_groups.append(g)\n",
    "            tweet_id_group.clear()\n",
    "            counter = 0\n",
    "    # add any lingering ids\n",
    "    if len(tweet_id_group) > 0:\n",
    "        g = list(tweet_id_group)\n",
    "        tweet_id_groups.append(tweet_id_group)\n",
    "    # write the json to the file\n",
    "    for index in range(len(tweet_id_groups)):\n",
    "        fetch_tweets(tweet_id_groups[index])\n",
    "    # write out the beginning of the file\n",
    "    with open(\"/home/workspace/tweet_json.txt\", \"w\") as output:\n",
    "        output.write(\"[\")\n",
    "        output.close()\n",
    "    # write out the rows to a file\n",
    "    with open(\"/home/workspace/tweet_json.txt\", \"a\") as output:\n",
    "        for index, line in enumerate(tweet_json_list):\n",
    "            output.write(str(line))\n",
    "            if index < len(tweet_json_list)-1:\n",
    "                output.write(\",\")\n",
    "        output.write(\"]\")\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the tweet json into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the tweet data from Twitter API if we don't already have it...\n",
    "my_file = Path(\"/home/workspace/tweet_json.txt\")\n",
    "if my_file.is_file() == True:\n",
    "    with open('/home/workspace/tweet_json.txt') as json_data:\n",
    "        tweet_json_raw = json.load(json_data)\n",
    "    tweet_json_raw = json.dumps(tweet_json_raw)\n",
    "    tweet_json = pd.read_json(tweet_json_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's examine the 'variable columns' to see if they can be melted into a single 'dog_type' column\n",
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    count=0\n",
    "    if row['doggo'] != 'None':\n",
    "        count+=1\n",
    "    if row['floofer'] != 'None':\n",
    "        count+=1\n",
    "    if row['pupper'] != 'None':\n",
    "        count+=1\n",
    "    if row['puppo'] != 'None':\n",
    "        count+=1\n",
    "    if count >=2:\n",
    "        print(\"tweet id \" + str(row['tweet_id']) + \" has multiple dog types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There were four tweet ids that had multiple dog types, and upon visual examination it appears that the situation of a single dog having multiple dog types is legitimate. However, examination did reveal one tweet (Tweet ID: 759793422261743616) that contained two dogs. This is essentially two records in one, and will need to be manually removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Quality Issues\n",
    "\n",
    "### `tweet archive` table:\n",
    " - some records are retweets (not original ratings)\n",
    " - some records contain multiple dogs (multiple records in one)\n",
    " - some dog names are incorrect or absent ('None')\n",
    " - columns `doggo`, `floofer`, `pupper`, `puppo` would be better represented as boolean\n",
    " - there are empty values (NaN) in the `breed` column *this issue was added later in the process, iteratively.\n",
    " - date/time data occurs as a raw timestamp\n",
    "\n",
    " \n",
    "### `image prediction` table:\n",
    " - there are tweets with no detected dog\n",
    " - column `p1_conf` values have varying number of digits.\n",
    " - column `p2_conf` values have varying number of digits.\n",
    " - column `p3_conf` values have varying number of digits.\n",
    "\n",
    "\n",
    "\n",
    "## Tidiness Issues\n",
    "\n",
    "### `tweet archive` table:\n",
    " - table could be improved by adding a `breed` column\n",
    " - there are unused columns\n",
    " - data in `tweet archive` and `image predictions`\n",
    " - column `retweet_count` in table `tweet json` is isolated from the other datasets\n",
    " \n",
    " \n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean = twitter_archive.copy()\n",
    "image_predictions_clean = image_predictions.copy()\n",
    "tweet_json_clean = tweet_json.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we summarize the cleaning tasks that will be carried out below:\n",
    "\n",
    "## Quality Issues\n",
    "\n",
    "### `tweet archive` table:\n",
    " - remove records that are retweets\n",
    " - remove records that contain incorrect or absent dog names\n",
    " - manually remove records containing multiple dogs (tweet_id = 759793422261743616)\n",
    " - re-format column values for `doggo`, `floofer`, `pupper`, `puppo` to boolean (true/false)\n",
    " - remove empty breeds (NaN)\n",
    " - convert raw timestamp into formatted date/time\n",
    "\n",
    " \n",
    "### `image prediction` table:\n",
    " - remove tweets with no detected dog\n",
    " - round column `p1_conf` values to nearest hundredth\n",
    " - round column `p2_conf` values to nearest hundredth\n",
    " - round column `p3_conf` values to nearest hundredth\n",
    "\n",
    "\n",
    "## Tidiness Issues\n",
    "\n",
    "### `tweet archive` table:\n",
    " - create `breed` column from image predictions table breed values `p1` `p2` `p3`.\n",
    " - remove unused columns\n",
    " - add column `retweet_count` from table `tweet json` to `tweet archive` table\n",
    " - inner join the tables `tweet archive` and `image predictions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`tweet archive` remove records that are retweets\n",
    "\n",
    "#### Define\n",
    "Identify tweets that have a value in their `retweeted_status_id` column, and drop it from the dataset.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if not np.isnan(row['retweeted_status_id']):\n",
    "        twitter_archive_clean.drop(index, inplace=True)\n",
    "        \n",
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if not np.isnan(row['retweeted_status_user_id']):\n",
    "        twitter_archive_clean.drop(index, inplace=True)\n",
    "        \n",
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if not np.isnan(row['retweeted_status_timestamp']):\n",
    "        twitter_archive_clean.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if not np.isnan(row['retweeted_status_id']):\n",
    "        print(\"dataset still contains retweets\")\n",
    "        \n",
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if not np.isnan(row['retweeted_status_user_id']):\n",
    "        print(\"dataset still contains retweets\")\n",
    "        \n",
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if not np.isnan(row['retweeted_status_timestamp']):\n",
    "        print(\"dataset still contains retweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`tweet archive` remove records that contain incorrect or absent dog names\n",
    "\n",
    "#### Define\n",
    "Identify and remove tweets that have a `name` value of 'None' or a value that is lowercase (lowercase values are often mistakenly processed non-name words like 'a', 'an', 'the', etc).\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if row['name'] == 'None' or row['name'].islower():\n",
    "        twitter_archive_clean.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if row['name'] == 'None' or row['name'].islower():\n",
    "        print(\"dataset still contains incorrect names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`tweet archive` manually remove records containing multiple dogs (tweet_id = 759793422261743616)\n",
    "\n",
    "#### Define\n",
    "Remove the data point with tweet_id of 759793422261743616, since it contains two dogs in a single data point.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean.tweet_id != 759793422261743616]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if row['tweet_id'] == 759793422261743616:\n",
    "        print(\"dataset still contains data point where tweet_id = 759793422261743616\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`tweet archive` re-format column values for `doggo`, `floofer`, `pupper`, `puppo` to boolean (true/false)\n",
    "\n",
    "\n",
    "#### Define\n",
    "Iterate over each of the columns `doggo`, `floofer`, `pupper`, `puppo` and if the value is 'None' set to a boolean value of False, otherwise set to True.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if row['doggo'] == 'None':\n",
    "        twitter_archive_clean.set_value(index, 'doggo', False)\n",
    "    else:\n",
    "        twitter_archive_clean.set_value(index, 'doggo', True)\n",
    "    if row['floofer'] == 'None':\n",
    "        twitter_archive_clean.set_value(index, 'floofer', False)\n",
    "    else:\n",
    "        twitter_archive_clean.set_value(index, 'floofer', True)\n",
    "    if row['pupper'] == 'None':\n",
    "        twitter_archive_clean.set_value(index, 'pupper', False)\n",
    "    else:\n",
    "        twitter_archive_clean.set_value(index, 'pupper', True)\n",
    "    if row['puppo'] == 'None':\n",
    "        twitter_archive_clean.set_value(index, 'puppo', False)\n",
    "    else:\n",
    "        twitter_archive_clean.set_value(index, 'puppo', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if row['doggo'] != True and row['doggo'] != False:\n",
    "        print(\"tweet \" + str(row['tweet_id']) + \" has an incorrect doggo value\")\n",
    "    if row['floofer'] != True and row['floofer'] != False:\n",
    "        print(\"tweet \" + str(row['tweet_id']) + \" has an incorrect floofer value\")\n",
    "    if row['pupper'] != True and row['pupper'] != False:\n",
    "        print(\"tweet \" + str(row['tweet_id']) + \" has an incorrect pupper value\")\n",
    "    if row['puppo'] != True and row['puppo'] != False:\n",
    "        print(\"tweet \" + str(row['tweet_id']) + \" has an incorrect puppo value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`image predictions`  - remove tweets with no detected dog\n",
    "\n",
    "\n",
    "#### Define\n",
    "Remove all data points in which all three detection columns (`p1_dog`, `p2_dog`, `p3_dog`) are False.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in image_predictions_clean.iterrows():\n",
    "    if row['p1_dog'] == False and row['p2_dog'] == False and row['p3_dog'] == False:\n",
    "        image_predictions_clean.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in image_predictions_clean.iterrows():\n",
    "    if row['p1_dog'] == False and row['p2_dog'] == False and row['p3_dog'] == False:\n",
    "        print(\"tweet \" + str(row['tweet_id']) + \" failed to detect a dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`image predictions`  - round columns `p1_conf`, `p2_conf`, `p3_conf` values to nearest hundredth\n",
    "\n",
    "\n",
    "#### Define\n",
    "Round values in column `p1_conf`, `p2_conf`, `p3_conf` to the nearest hundredth.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimal_places = 4 \n",
    "image_predictions_clean['p1_conf'] = image_predictions_clean['p1_conf'].apply(lambda x: round(x,decimal_places))\n",
    "image_predictions_clean['p2_conf'] = image_predictions_clean['p2_conf'].apply(lambda x: round(x,decimal_places))\n",
    "image_predictions_clean['p3_conf'] = image_predictions_clean['p3_conf'].apply(lambda x: round(x,decimal_places))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "for index, row in image_predictions_clean.iterrows():\n",
    "    e1 = abs(decimal.Decimal(str(row['p1_conf'])).as_tuple().exponent)\n",
    "    e2 = abs(decimal.Decimal(str(row['p2_conf'])).as_tuple().exponent)\n",
    "    e3 = abs(decimal.Decimal(str(row['p3_conf'])).as_tuple().exponent)\n",
    "    if(e1 > 2):\n",
    "        print(\"tweet \" + str(row['tweet_id']) + \" has a p1_conf value that is not rounded (\" + str(e1) + \" digits past decimal)\")\n",
    "    if(e2 > 2):\n",
    "        print(\"tweet \" + str(row['tweet_id']) + \" has a p2_conf value that is not rounded (\" + str(e2) + \" digits past decimal)\")\n",
    "    if(e2 > 3):\n",
    "        print(\"tweet \" + str(row['tweet_id']) + \" has a p3_conf value that is not rounded (\" + str(e3) + \" digits past decimal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`tweet archive` - create `breed` column from image predictions table breed values `p1` `p2` `p3`.\n",
    "\n",
    "#### Define\n",
    "Create `breed` column from image predictions table `p1` `p2` `p3` taking the postivie dog detection of the highest confidence value. First evaluate `p1` and if it is false, then evalueate `p2` and so on.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    id_df = image_predictions_clean.loc[image_predictions_clean['tweet_id'] == row['tweet_id']]\n",
    "    if id_df.empty == False:\n",
    "        p1 = id_df['p1'].values[0]\n",
    "        p2 = id_df['p2'].values[0]\n",
    "        p3 = id_df['p3'].values[0]\n",
    "        p1_is_dog = id_df['p1_dog'].values[0]\n",
    "        p2_is_dog = id_df['p2_dog'].values[0]\n",
    "        p3_is_dog = id_df['p3_dog'].values[0]\n",
    "        if p1_is_dog == True:\n",
    "            twitter_archive_clean.set_value(index, 'breed', p1)\n",
    "        elif p2_is_dog == True:\n",
    "            twitter_archive_clean.set_value(index, 'breed', p2)\n",
    "        elif p3_is_dog == True:\n",
    "            twitter_archive_clean.set_value(index, 'breed', p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "Let's see if the field was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the breed column was created, but there were some data points that still don't have a breed (NaN) so these should be removed. Let's add another quality task to remove empty (NaN) breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`tweet archive` - remove empty breeds (NaN)\n",
    "\n",
    "#### Define\n",
    "Identify and remove tweets an empty value (NaN) in the `breed` column.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if row['breed'] == 'NaN':\n",
    "        twitter_archive_clean.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    if row['breed'] == 'NaN':\n",
    "        print('tweet ' + str(row['tweet_id']) + \" has an empty breed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's also do a visual inspection to be sure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`tweet archive` - remove unused columns\n",
    "\n",
    "#### Define\n",
    "Remove all columns that are not useful to our data analysis purposes. Specifically, let's get rid of `in_reply_to_status_id`, `in_reply_to_user_id`, `source`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp`, `expanded_urls`.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.drop('in_reply_to_status_id', axis=1, inplace = True)\n",
    "twitter_archive_clean.drop('in_reply_to_user_id', axis=1, inplace = True)\n",
    "twitter_archive_clean.drop('source', axis=1, inplace = True)\n",
    "twitter_archive_clean.drop('retweeted_status_id', axis=1, inplace = True)\n",
    "twitter_archive_clean.drop('retweeted_status_user_id', axis=1, inplace = True)\n",
    "twitter_archive_clean.drop('retweeted_status_timestamp', axis=1, inplace = True)\n",
    "twitter_archive_clean.drop('expanded_urls', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`tweet archive` - add column `retweet_count` from table `tweet json`\n",
    "\n",
    "\n",
    "#### Define\n",
    "Add the column `retweet_count` from table `tweet json` to the table `tweet archive`.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the column\n",
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    tj_df = tweet_json_clean.loc[tweet_json_clean['id_str'] == row['tweet_id']]\n",
    "    if tj_df.empty == False:\n",
    "        rtc = tj_df['retweet_count'].values[0]\n",
    "        if np.isnan(rtc):\n",
    "            rtc = 0\n",
    "        twitter_archive_clean.set_value(index, 'retweet_count', rtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up bad NaN values\n",
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    value = row['retweet_count']\n",
    "    if np.isnan(value):\n",
    "        twitter_archive_clean.set_value(index, 'retweet_count', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \n",
    "for index, row in twitter_archive_clean.iterrows():\n",
    "    value = row['retweet_count']\n",
    "    val_int = int(value)\n",
    "    if type(value) != int:\n",
    "        twitter_archive_clean.set_value(index, 'retweet_count', val_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to integer data type\n",
    "twitter_archive_clean[\"retweet_count\"] = twitter_archive_clean['retweet_count'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "Let's see if the new column was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`tweet archive` - inner join the tables `tweet archive` and `image predictions`\n",
    "\n",
    "#### Define\n",
    "Merge the data in the tables `tweet archive` and `image predictions` using an inner join.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_master = pd.merge(twitter_archive_clean,image_predictions_clean, on=['tweet_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Now let's do some visual analysis to see what we can find out about this dataset. Particularly, we're interested in which breeds are most common in this dataset. Let's start by breaking down the counts of the categorical variable `breed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total count of all datapoints\n",
    "total_datapoint_count = len(twitter_archive_clean)\n",
    "\n",
    "# get counts of the categorial variable 'breed'\n",
    "breed_df = pd.DataFrame(twitter_archive_clean['breed'].value_counts())\n",
    "breed_count = len(breed_df)\n",
    "subtract = breed_count - 5\n",
    "\n",
    "print(str(breed_count) + \" breeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at some of these breeds\n",
    "print(breed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breed_df.drop(breed_df.tail(subtract).index,inplace=True)\n",
    "\n",
    "name_list = []\n",
    "values_list = []\n",
    "named_datapoint_count = 0\n",
    "for index, row in breed_df.iterrows():\n",
    "    name_list.append(index)\n",
    "    values_list.append(row['breed'])\n",
    "    named_datapoint_count+=row['breed']\n",
    "    \n",
    "other_count = total_datapoint_count - named_datapoint_count\n",
    "\n",
    "# add counts for other breeds\n",
    "name_list.append('Other')\n",
    "values_list.append(other_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = name_list\n",
    "sizes = values_list\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "ax1.axis('equal') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    " - There are 110 different breeds in the dataset.\n",
    " - The 5 most frequent breeds are Golden Retriever, Labrador Retriever, Pembroke, Chihuahua and Pug.\n",
    " - The most frequent breed is Golden Retriever (7.5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the cleaned dataframe as a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_master.to_csv(\"/home/workspace/twitter_archive_master.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    " - https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas (Dataframe row iteration code example)\n",
    " - https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data (Dataframe csv reader code example)\n",
    " - https://stackoverflow.com/questions/6189956/easy-way-of-finding-decimal-places (Testing decimal points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
